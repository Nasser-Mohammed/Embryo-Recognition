{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "01df8641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "513a4691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "cc103ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.cnn_layer = nn.Sequential(\n",
    "            nn.Conv2d(1, 4, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "            nn.Conv2d(4, 4, kernel_size = 3, stride = 1, padding = 1),\n",
    "            nn.BatchNorm2d(4),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        )\n",
    "        self.linear_layer = nn.Sequential(nn.Linear(20200, 1))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        pred = self.cnn_layer(x)\n",
    "        pred = pred.view(pred.size(0), -1)\n",
    "        pred = self.linear_layer(pred)\n",
    "        return pred   \n",
    "    \n",
    "    def view_parameters(self):\n",
    "        print(self.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "f6abcbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(epochs, learnRate, model, x, y):\n",
    "    criterion = nn.MSELoss()  #mean square error loss\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = learnRate) #stochasitic gradient decent\n",
    "    for i in range(epochs):\n",
    "        y_pred = model.forward(x)\n",
    "        loss = criterion(y_pred, y)\n",
    "        optimizer.zero_grad() #clears the gradient so we can calculate it again\n",
    "        loss.backward()  #computes the gradient of the loss\n",
    "        optimizer.step() #takes the model in a step away from the direction of the gradient\n",
    "        print(\"Epoch: \" + str(i+1) + \" Loss: \" + str(loss) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5298232b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_6408/291247112.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\nikce\\AppData\\Local\\Temp/ipykernel_6408/291247112.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    def test_loop()\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def test_loop()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "edd4d6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: tensor(6.7734, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 2 Loss: tensor(333897.1875, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 3 Loss: tensor(7.2535e+14, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 4 Loss: tensor(2.8984e+11, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 5 Loss: tensor(2.7836e+11, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 6 Loss: tensor(2.6734e+11, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 7 Loss: tensor(2.5675e+11, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 8 Loss: tensor(2.4659e+11, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 9 Loss: tensor(2.3682e+11, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 10 Loss: tensor(2.2744e+11, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 11 Loss: tensor(2.1844e+11, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 12 Loss: tensor(2.0979e+11, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 13 Loss: tensor(2.0148e+11, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 14 Loss: tensor(1.9350e+11, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 15 Loss: tensor(1.8584e+11, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 16 Loss: tensor(1.7848e+11, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 17 Loss: tensor(1.7141e+11, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 18 Loss: tensor(1.6462e+11, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 19 Loss: tensor(1.5810e+11, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 20 Loss: tensor(1.5184e+11, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 21 Loss: tensor(1.4583e+11, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 22 Loss: tensor(1.4006e+11, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 23 Loss: tensor(1.3451e+11, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 24 Loss: tensor(1.2918e+11, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 25 Loss: tensor(1.2407e+11, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 26 Loss: tensor(1.1915e+11, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 27 Loss: tensor(1.1444e+11, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 28 Loss: tensor(1.0990e+11, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 29 Loss: tensor(1.0555e+11, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 30 Loss: tensor(1.0137e+11, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 31 Loss: tensor(9.7357e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 32 Loss: tensor(9.3502e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 33 Loss: tensor(8.9799e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 34 Loss: tensor(8.6243e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 35 Loss: tensor(8.2828e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 36 Loss: tensor(7.9548e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 37 Loss: tensor(7.6398e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 38 Loss: tensor(7.3372e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 39 Loss: tensor(7.0467e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 40 Loss: tensor(6.7676e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 41 Loss: tensor(6.4996e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 42 Loss: tensor(6.2423e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 43 Loss: tensor(5.9951e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 44 Loss: tensor(5.7577e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 45 Loss: tensor(5.5297e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 46 Loss: tensor(5.3107e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 47 Loss: tensor(5.1004e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 48 Loss: tensor(4.8984e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 49 Loss: tensor(4.7044e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 50 Loss: tensor(4.5181e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 51 Loss: tensor(4.3392e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 52 Loss: tensor(4.1674e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 53 Loss: tensor(4.0024e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 54 Loss: tensor(3.8439e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 55 Loss: tensor(3.6916e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 56 Loss: tensor(3.5455e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 57 Loss: tensor(3.4051e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 58 Loss: tensor(3.2702e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 59 Loss: tensor(3.1407e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 60 Loss: tensor(3.0163e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 61 Loss: tensor(2.8969e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 62 Loss: tensor(2.7822e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 63 Loss: tensor(2.6720e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 64 Loss: tensor(2.5662e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 65 Loss: tensor(2.4646e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 66 Loss: tensor(2.3670e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 67 Loss: tensor(2.2732e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 68 Loss: tensor(2.1832e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 69 Loss: tensor(2.0968e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 70 Loss: tensor(2.0137e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 71 Loss: tensor(1.9340e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 72 Loss: tensor(1.8574e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 73 Loss: tensor(1.7839e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 74 Loss: tensor(1.7132e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 75 Loss: tensor(1.6454e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 76 Loss: tensor(1.5802e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 77 Loss: tensor(1.5176e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 78 Loss: tensor(1.4575e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 79 Loss: tensor(1.3998e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 80 Loss: tensor(1.3444e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 81 Loss: tensor(1.2911e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 82 Loss: tensor(1.2400e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 83 Loss: tensor(1.1909e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 84 Loss: tensor(1.1438e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 85 Loss: tensor(1.0985e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 86 Loss: tensor(1.0550e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 87 Loss: tensor(1.0132e+10, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 88 Loss: tensor(9.7306e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 89 Loss: tensor(9.3453e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 90 Loss: tensor(8.9752e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 91 Loss: tensor(8.6198e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 92 Loss: tensor(8.2785e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 93 Loss: tensor(7.9506e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 94 Loss: tensor(7.6358e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 95 Loss: tensor(7.3334e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 96 Loss: tensor(7.0430e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 97 Loss: tensor(6.7641e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 98 Loss: tensor(6.4962e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 99 Loss: tensor(6.2390e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 100 Loss: tensor(5.9919e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 101 Loss: tensor(5.7546e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 102 Loss: tensor(5.5268e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 103 Loss: tensor(5.3079e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 104 Loss: tensor(5.0977e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 105 Loss: tensor(4.8958e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 106 Loss: tensor(4.7020e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 107 Loss: tensor(4.5158e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 108 Loss: tensor(4.3369e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 109 Loss: tensor(4.1652e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 110 Loss: tensor(4.0003e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 111 Loss: tensor(3.8418e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 112 Loss: tensor(3.6897e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 113 Loss: tensor(3.5436e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 114 Loss: tensor(3.4033e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 115 Loss: tensor(3.2685e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 116 Loss: tensor(3.1391e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 117 Loss: tensor(3.0148e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 118 Loss: tensor(2.8954e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 119 Loss: tensor(2.7807e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 120 Loss: tensor(2.6706e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 121 Loss: tensor(2.5648e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 122 Loss: tensor(2.4633e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 123 Loss: tensor(2.3657e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 124 Loss: tensor(2.2721e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 125 Loss: tensor(2.1821e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 126 Loss: tensor(2.0957e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 127 Loss: tensor(2.0127e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 128 Loss: tensor(1.9330e+09, grad_fn=<MseLossBackward0>)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 129 Loss: tensor(1.8564e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 130 Loss: tensor(1.7829e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 131 Loss: tensor(1.7123e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 132 Loss: tensor(1.6445e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 133 Loss: tensor(1.5794e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 134 Loss: tensor(1.5168e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 135 Loss: tensor(1.4568e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 136 Loss: tensor(1.3991e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 137 Loss: tensor(1.3437e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 138 Loss: tensor(1.2905e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 139 Loss: tensor(1.2394e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 140 Loss: tensor(1.1903e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 141 Loss: tensor(1.1432e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 142 Loss: tensor(1.0979e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 143 Loss: tensor(1.0544e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 144 Loss: tensor(1.0127e+09, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 145 Loss: tensor(9.7255e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 146 Loss: tensor(9.3404e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 147 Loss: tensor(8.9705e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 148 Loss: tensor(8.6153e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 149 Loss: tensor(8.2741e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 150 Loss: tensor(7.9465e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 151 Loss: tensor(7.6318e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 152 Loss: tensor(7.3296e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 153 Loss: tensor(7.0393e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 154 Loss: tensor(6.7606e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 155 Loss: tensor(6.4928e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 156 Loss: tensor(6.2357e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 157 Loss: tensor(5.9888e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 158 Loss: tensor(5.7516e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 159 Loss: tensor(5.5239e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 160 Loss: tensor(5.3051e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 161 Loss: tensor(5.0950e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 162 Loss: tensor(4.8933e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 163 Loss: tensor(4.6995e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 164 Loss: tensor(4.5134e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 165 Loss: tensor(4.3347e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 166 Loss: tensor(4.1630e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 167 Loss: tensor(3.9982e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 168 Loss: tensor(3.8398e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 169 Loss: tensor(3.6878e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 170 Loss: tensor(3.5417e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 171 Loss: tensor(3.4015e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 172 Loss: tensor(3.2668e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 173 Loss: tensor(3.1374e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 174 Loss: tensor(3.0132e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 175 Loss: tensor(2.8939e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 176 Loss: tensor(2.7793e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 177 Loss: tensor(2.6692e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 178 Loss: tensor(2.5635e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 179 Loss: tensor(2.4620e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 180 Loss: tensor(2.3645e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 181 Loss: tensor(2.2709e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 182 Loss: tensor(2.1809e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 183 Loss: tensor(2.0946e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 184 Loss: tensor(2.0116e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 185 Loss: tensor(1.9320e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 186 Loss: tensor(1.8555e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 187 Loss: tensor(1.7820e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 188 Loss: tensor(1.7114e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 189 Loss: tensor(1.6436e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 190 Loss: tensor(1.5786e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 191 Loss: tensor(1.5160e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 192 Loss: tensor(1.4560e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 193 Loss: tensor(1.3984e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 194 Loss: tensor(1.3430e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 195 Loss: tensor(1.2898e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 196 Loss: tensor(1.2387e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 197 Loss: tensor(1.1897e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 198 Loss: tensor(1.1426e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 199 Loss: tensor(1.0973e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 200 Loss: tensor(1.0539e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 201 Loss: tensor(1.0121e+08, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 202 Loss: tensor(97204408., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 203 Loss: tensor(93355120., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 204 Loss: tensor(89658240., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 205 Loss: tensor(86107776., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 206 Loss: tensor(82697912., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 207 Loss: tensor(79423056., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 208 Loss: tensor(76277904., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 209 Loss: tensor(73257296., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 210 Loss: tensor(70356304., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 211 Loss: tensor(67570192., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 212 Loss: tensor(64894408., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 213 Loss: tensor(62324600., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 214 Loss: tensor(59856536., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 215 Loss: tensor(57486216., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 216 Loss: tensor(55209768., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 217 Loss: tensor(53023460., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 218 Loss: tensor(50923740., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 219 Loss: tensor(48907160., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 220 Loss: tensor(46970436., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 221 Loss: tensor(45110404., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 222 Loss: tensor(43324036., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 223 Loss: tensor(41608408., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 224 Loss: tensor(39960708., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 225 Loss: tensor(38378264., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 226 Loss: tensor(36858484., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 227 Loss: tensor(35398884., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 228 Loss: tensor(33997088., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 229 Loss: tensor(32650804., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 230 Loss: tensor(31357828., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 231 Loss: tensor(30116060., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 232 Loss: tensor(28923460., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 233 Loss: tensor(27778092., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 234 Loss: tensor(26678082., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 235 Loss: tensor(25621630., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 236 Loss: tensor(24607010., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 237 Loss: tensor(23632574., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 238 Loss: tensor(22696722., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 239 Loss: tensor(21797934., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 240 Loss: tensor(20934736., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 241 Loss: tensor(20105724., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 242 Loss: tensor(19309536., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 243 Loss: tensor(18544878., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 244 Loss: tensor(17810500., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 245 Loss: tensor(17105210., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 246 Loss: tensor(16427842., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 247 Loss: tensor(15777298., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 248 Loss: tensor(15152518., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 249 Loss: tensor(14552478., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 250 Loss: tensor(13976200., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 251 Loss: tensor(13422742., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 252 Loss: tensor(12891201., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 253 Loss: tensor(12380711., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 254 Loss: tensor(11890433., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 255 Loss: tensor(11419570., grad_fn=<MseLossBackward0>)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 256 Loss: tensor(10967358., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 257 Loss: tensor(10533049., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 258 Loss: tensor(10115942., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 259 Loss: tensor(9715351., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 260 Loss: tensor(9330623., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 261 Loss: tensor(8961129., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 262 Loss: tensor(8606270., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 263 Loss: tensor(8265461., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 264 Loss: tensor(7938147., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 265 Loss: tensor(7623799., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 266 Loss: tensor(7321895., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 267 Loss: tensor(7031949., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 268 Loss: tensor(6753484., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 269 Loss: tensor(6486046.5000, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 270 Loss: tensor(6229198.5000, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 271 Loss: tensor(5982521.5000, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 272 Loss: tensor(5745613., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 273 Loss: tensor(5518087.5000, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 274 Loss: tensor(5299571.5000, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 275 Loss: tensor(5089708.5000, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 276 Loss: tensor(4888156., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 277 Loss: tensor(4694585., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 278 Loss: tensor(4508680., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 279 Loss: tensor(4330136.5000, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 280 Loss: tensor(4158663.5000, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 281 Loss: tensor(3993979.5000, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 282 Loss: tensor(3835818.5000, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 283 Loss: tensor(3683919.5000, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 284 Loss: tensor(3538036.5000, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 285 Loss: tensor(3397930.5000, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 286 Loss: tensor(3263372.2500, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 287 Loss: tensor(3134142.2500, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 288 Loss: tensor(3010030.2500, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 289 Loss: tensor(2890833.7500, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 290 Loss: tensor(2776356.2500, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 291 Loss: tensor(2666412.5000, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 292 Loss: tensor(2560822.5000, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 293 Loss: tensor(2459414.2500, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 294 Loss: tensor(2362021.7500, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 295 Loss: tensor(2268485.5000, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 296 Loss: tensor(2178653.7500, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 297 Loss: tensor(2092378.7500, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 298 Loss: tensor(2009520.2500, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 299 Loss: tensor(1929943.3750, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 300 Loss: tensor(1853517.7500, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 301 Loss: tensor(1780118.2500, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 302 Loss: tensor(1709625.7500, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 303 Loss: tensor(1641924.8750, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 304 Loss: tensor(1576904.2500, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 305 Loss: tensor(1514458.8750, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 306 Loss: tensor(1454486.2500, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 307 Loss: tensor(1396888.8750, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 308 Loss: tensor(1341571.8750, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 309 Loss: tensor(1288445.7500, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 310 Loss: tensor(1237423.1250, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 311 Loss: tensor(1188421.1250, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 312 Loss: tensor(1141359.7500, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 313 Loss: tensor(1096161.8750, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 314 Loss: tensor(1052753.7500, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 315 Loss: tensor(1011064.6250, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 316 Loss: tensor(971026.3750, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 317 Loss: tensor(932573.8750, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 318 Loss: tensor(895643.8750, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 319 Loss: tensor(860176.3750, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 320 Loss: tensor(826113.4375, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 321 Loss: tensor(793399.4375, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 322 Loss: tensor(761980.8125, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 323 Loss: tensor(731806.3750, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 324 Loss: tensor(702826.8750, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 325 Loss: tensor(674994.8750, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 326 Loss: tensor(648265., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 327 Loss: tensor(622593.8125, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 328 Loss: tensor(597939., grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 329 Loss: tensor(574260.5625, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 330 Loss: tensor(551519.8750, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 331 Loss: tensor(529679.5625, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 332 Loss: tensor(508704.1875, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 333 Loss: tensor(488559.5625, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 334 Loss: tensor(469212.5625, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 335 Loss: tensor(450631.6875, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 336 Loss: tensor(432786.6875, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 337 Loss: tensor(415648.3125, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 338 Loss: tensor(399188.6875, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 339 Loss: tensor(383380.7500, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 340 Loss: tensor(368198.9375, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 341 Loss: tensor(353618.2188, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 342 Loss: tensor(339614.9375, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 343 Loss: tensor(326166.2188, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 344 Loss: tensor(313250.0312, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 345 Loss: tensor(300845.3125, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 346 Loss: tensor(288931.9375, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 347 Loss: tensor(277490.2188, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 348 Loss: tensor(266501.5625, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 349 Loss: tensor(255948.0938, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 350 Loss: tensor(245812.5781, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 351 Loss: tensor(236078.4062, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 352 Loss: tensor(226729.7188, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 353 Loss: tensor(217751.2188, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 354 Loss: tensor(209128.2500, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 355 Loss: tensor(200846.7656, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 356 Loss: tensor(192893.2344, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 357 Loss: tensor(185254.6406, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 358 Loss: tensor(177918.5312, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 359 Loss: tensor(170872.9844, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 360 Loss: tensor(164106.3906, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 361 Loss: tensor(157607.7656, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 362 Loss: tensor(151366.5156, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 363 Loss: tensor(145372.3906, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 364 Loss: tensor(139615.6406, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 365 Loss: tensor(134086.8906, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 366 Loss: tensor(128777.0469, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 367 Loss: tensor(123677.4844, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 368 Loss: tensor(118779.8594, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 369 Loss: tensor(114076.1719, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 370 Loss: tensor(109558.7656, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 371 Loss: tensor(105220.2344, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 372 Loss: tensor(101053.5000, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 373 Loss: tensor(97051.7656, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 374 Loss: tensor(93208.5234, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 375 Loss: tensor(89517.4766, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 376 Loss: tensor(85972.5781, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 377 Loss: tensor(82568.0625, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 378 Loss: tensor(79298.3750, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 379 Loss: tensor(76158.1719, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 380 Loss: tensor(73142.3047, grad_fn=<MseLossBackward0>)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 381 Loss: tensor(70245.8672, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 382 Loss: tensor(67464.1328, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 383 Loss: tensor(64792.5547, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 384 Loss: tensor(62226.7695, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 385 Loss: tensor(59762.5859, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 386 Loss: tensor(57395.9922, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 387 Loss: tensor(55123.1172, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 388 Loss: tensor(52940.2422, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 389 Loss: tensor(50843.8047, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 390 Loss: tensor(48830.3828, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 391 Loss: tensor(46896.6953, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 392 Loss: tensor(45039.5898, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 393 Loss: tensor(43256.0234, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 394 Loss: tensor(41543.0859, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 395 Loss: tensor(39897.9727, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 396 Loss: tensor(38318.0117, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 397 Loss: tensor(36800.6211, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 398 Loss: tensor(35343.3164, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 399 Loss: tensor(33943.7227, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 400 Loss: tensor(32599.5508, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 401 Loss: tensor(31308.6055, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 402 Loss: tensor(30068.7852, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 403 Loss: tensor(28878.0664, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 404 Loss: tensor(27734.4961, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 405 Loss: tensor(26636.2070, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 406 Loss: tensor(25581.4121, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 407 Loss: tensor(24568.3906, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 408 Loss: tensor(23595.4844, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 409 Loss: tensor(22661.1055, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 410 Loss: tensor(21763.7207, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 411 Loss: tensor(20901.8789, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 412 Loss: tensor(20074.1699, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 413 Loss: tensor(19279.2324, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 414 Loss: tensor(18515.7754, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 415 Loss: tensor(17782.5488, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 416 Loss: tensor(17078.3613, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 417 Loss: tensor(16402.0605, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 418 Loss: tensor(15752.5371, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 419 Loss: tensor(15128.7363, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 420 Loss: tensor(14529.6387, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 421 Loss: tensor(13954.2656, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 422 Loss: tensor(13401.6777, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 423 Loss: tensor(12870.9697, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 424 Loss: tensor(12361.2803, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 425 Loss: tensor(11871.7725, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 426 Loss: tensor(11401.6494, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 427 Loss: tensor(10950.1465, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 428 Loss: tensor(10516.5215, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 429 Loss: tensor(10100.0654, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 430 Loss: tensor(9700.1025, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 431 Loss: tensor(9315.9814, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 432 Loss: tensor(8947.0654, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 433 Loss: tensor(8592.7646, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 434 Loss: tensor(8252.4912, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 435 Loss: tensor(7925.6904, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 436 Loss: tensor(7611.8350, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 437 Loss: tensor(7310.4053, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 438 Loss: tensor(7020.9131, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 439 Loss: tensor(6742.8838, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 440 Loss: tensor(6475.8662, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 441 Loss: tensor(6219.4214, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 442 Loss: tensor(5973.1323, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 443 Loss: tensor(5736.5952, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 444 Loss: tensor(5509.4263, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 445 Loss: tensor(5291.2534, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 446 Loss: tensor(5081.7192, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 447 Loss: tensor(4880.4839, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 448 Loss: tensor(4687.2158, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 449 Loss: tensor(4501.6025, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 450 Loss: tensor(4323.3394, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 451 Loss: tensor(4152.1357, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 452 Loss: tensor(3987.7104, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 453 Loss: tensor(3829.7974, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 454 Loss: tensor(3678.1372, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 455 Loss: tensor(3532.4839, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 456 Loss: tensor(3392.5972, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 457 Loss: tensor(3258.2502, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 458 Loss: tensor(3129.2234, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 459 Loss: tensor(3005.3064, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 460 Loss: tensor(2886.2966, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 461 Loss: tensor(2771.9993, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 462 Loss: tensor(2662.2283, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 463 Loss: tensor(2556.8040, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 464 Loss: tensor(2455.5544, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 465 Loss: tensor(2358.3145, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 466 Loss: tensor(2264.9255, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 467 Loss: tensor(2175.2344, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 468 Loss: tensor(2089.0952, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 469 Loss: tensor(2006.3669, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 470 Loss: tensor(1926.9148, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 471 Loss: tensor(1850.6091, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 472 Loss: tensor(1777.3245, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 473 Loss: tensor(1706.9426, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 474 Loss: tensor(1639.3479, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 475 Loss: tensor(1574.4298, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 476 Loss: tensor(1512.0822, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 477 Loss: tensor(1452.2040, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 478 Loss: tensor(1394.6964, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 479 Loss: tensor(1339.4664, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 480 Loss: tensor(1286.4235, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 481 Loss: tensor(1235.4812, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 482 Loss: tensor(1186.5559, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 483 Loss: tensor(1139.5685, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 484 Loss: tensor(1094.4415, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 485 Loss: tensor(1051.1016, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 486 Loss: tensor(1009.4779, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 487 Loss: tensor(969.5026, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 488 Loss: tensor(931.1102, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 489 Loss: tensor(894.2382, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 490 Loss: tensor(858.8263, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 491 Loss: tensor(824.8169, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 492 Loss: tensor(792.1542, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 493 Loss: tensor(760.7848, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 494 Loss: tensor(730.6578, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 495 Loss: tensor(701.7237, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 496 Loss: tensor(673.9355, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 497 Loss: tensor(647.2476, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 498 Loss: tensor(621.6166, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 499 Loss: tensor(597.0005, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 500 Loss: tensor(573.3594, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 501 Loss: tensor(550.6543, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 502 Loss: tensor(528.8484, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 503 Loss: tensor(507.9061, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 504 Loss: tensor(487.7929, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 505 Loss: tensor(468.4764, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 506 Loss: tensor(449.9246, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 507 Loss: tensor(432.1076, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 508 Loss: tensor(414.9962, grad_fn=<MseLossBackward0>)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 509 Loss: tensor(398.5623, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 510 Loss: tensor(382.7793, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 511 Loss: tensor(367.6212, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 512 Loss: tensor(353.0634, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 513 Loss: tensor(339.0821, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 514 Loss: tensor(325.6545, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 515 Loss: tensor(312.7585, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 516 Loss: tensor(300.3733, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 517 Loss: tensor(288.4785, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 518 Loss: tensor(277.0547, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 519 Loss: tensor(266.0834, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 520 Loss: tensor(255.5465, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 521 Loss: tensor(245.4269, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 522 Loss: tensor(235.7080, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 523 Loss: tensor(226.3739, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 524 Loss: tensor(217.4095, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 525 Loss: tensor(208.8001, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 526 Loss: tensor(200.5316, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 527 Loss: tensor(192.5906, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 528 Loss: tensor(184.9640, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 529 Loss: tensor(177.6394, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 530 Loss: tensor(170.6049, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 531 Loss: tensor(163.8490, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 532 Loss: tensor(157.3605, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 533 Loss: tensor(151.1290, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 534 Loss: tensor(145.1444, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 535 Loss: tensor(139.3966, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 536 Loss: tensor(133.8766, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 537 Loss: tensor(128.5750, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 538 Loss: tensor(123.4835, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 539 Loss: tensor(118.5935, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 540 Loss: tensor(113.8972, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 541 Loss: tensor(109.3869, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 542 Loss: tensor(105.0552, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 543 Loss: tensor(100.8950, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 544 Loss: tensor(96.8995, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 545 Loss: tensor(93.0623, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 546 Loss: tensor(89.3770, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 547 Loss: tensor(85.8377, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 548 Loss: tensor(82.4385, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 549 Loss: tensor(79.1740, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 550 Loss: tensor(76.0387, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 551 Loss: tensor(73.0275, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 552 Loss: tensor(70.1356, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 553 Loss: tensor(67.3583, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 554 Loss: tensor(64.6909, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 555 Loss: tensor(62.1291, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 556 Loss: tensor(59.6688, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 557 Loss: tensor(57.3059, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 558 Loss: tensor(55.0366, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 559 Loss: tensor(52.8572, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 560 Loss: tensor(50.7640, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 561 Loss: tensor(48.7537, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 562 Loss: tensor(46.8231, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 563 Loss: tensor(44.9689, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 564 Loss: tensor(43.1881, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 565 Loss: tensor(41.4779, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 566 Loss: tensor(39.8354, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 567 Loss: tensor(38.2579, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 568 Loss: tensor(36.7429, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 569 Loss: tensor(35.2878, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 570 Loss: tensor(33.8904, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 571 Loss: tensor(32.5484, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 572 Loss: tensor(31.2595, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 573 Loss: tensor(30.0216, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 574 Loss: tensor(28.8327, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 575 Loss: tensor(27.6910, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 576 Loss: tensor(26.5944, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 577 Loss: tensor(25.5413, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 578 Loss: tensor(24.5298, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 579 Loss: tensor(23.5584, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 580 Loss: tensor(22.6255, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 581 Loss: tensor(21.7296, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 582 Loss: tensor(20.8691, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 583 Loss: tensor(20.0427, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 584 Loss: tensor(19.2490, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 585 Loss: tensor(18.4867, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 586 Loss: tensor(17.7546, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 587 Loss: tensor(17.0515, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 588 Loss: tensor(16.3763, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 589 Loss: tensor(15.7278, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 590 Loss: tensor(15.1050, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 591 Loss: tensor(14.5068, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 592 Loss: tensor(13.9324, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 593 Loss: tensor(13.3806, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 594 Loss: tensor(12.8508, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 595 Loss: tensor(12.3419, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 596 Loss: tensor(11.8531, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 597 Loss: tensor(11.3838, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 598 Loss: tensor(10.9330, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 599 Loss: tensor(10.5000, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 600 Loss: tensor(10.0842, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 601 Loss: tensor(9.6849, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 602 Loss: tensor(9.3014, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 603 Loss: tensor(8.9330, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 604 Loss: tensor(8.5793, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 605 Loss: tensor(8.2395, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 606 Loss: tensor(7.9132, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 607 Loss: tensor(7.5999, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 608 Loss: tensor(7.2989, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 609 Loss: tensor(7.0099, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 610 Loss: tensor(6.7323, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 611 Loss: tensor(6.4657, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 612 Loss: tensor(6.2097, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 613 Loss: tensor(5.9638, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 614 Loss: tensor(5.7276, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 615 Loss: tensor(5.5008, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 616 Loss: tensor(5.2829, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 617 Loss: tensor(5.0737, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 618 Loss: tensor(4.8728, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 619 Loss: tensor(4.6799, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 620 Loss: tensor(4.4945, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 621 Loss: tensor(4.3165, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 622 Loss: tensor(4.1456, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 623 Loss: tensor(3.9814, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 624 Loss: tensor(3.8238, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 625 Loss: tensor(3.6724, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 626 Loss: tensor(3.5269, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 627 Loss: tensor(3.3873, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 628 Loss: tensor(3.2531, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 629 Loss: tensor(3.1243, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 630 Loss: tensor(3.0006, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 631 Loss: tensor(2.8818, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 632 Loss: tensor(2.7676, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 633 Loss: tensor(2.6580, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 634 Loss: tensor(2.5528, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 635 Loss: tensor(2.4517, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 636 Loss: tensor(2.3546, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 637 Loss: tensor(2.2614, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 638 Loss: tensor(2.1718, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 639 Loss: tensor(2.0858, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 640 Loss: tensor(2.0032, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 641 Loss: tensor(1.9239, grad_fn=<MseLossBackward0>)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 642 Loss: tensor(1.8477, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 643 Loss: tensor(1.7745, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 644 Loss: tensor(1.7043, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 645 Loss: tensor(1.6368, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 646 Loss: tensor(1.5720, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 647 Loss: tensor(1.5097, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 648 Loss: tensor(1.4499, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 649 Loss: tensor(1.3925, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 650 Loss: tensor(1.3374, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 651 Loss: tensor(1.2844, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 652 Loss: tensor(1.2335, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 653 Loss: tensor(1.1847, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 654 Loss: tensor(1.1378, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 655 Loss: tensor(1.0927, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 656 Loss: tensor(1.0495, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 657 Loss: tensor(1.0079, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 658 Loss: tensor(0.9680, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 659 Loss: tensor(0.9296, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 660 Loss: tensor(0.8928, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 661 Loss: tensor(0.8575, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 662 Loss: tensor(0.8235, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 663 Loss: tensor(0.7909, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 664 Loss: tensor(0.7596, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 665 Loss: tensor(0.7295, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 666 Loss: tensor(0.7006, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 667 Loss: tensor(0.6729, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 668 Loss: tensor(0.6462, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 669 Loss: tensor(0.6206, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 670 Loss: tensor(0.5961, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 671 Loss: tensor(0.5725, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 672 Loss: tensor(0.5498, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 673 Loss: tensor(0.5280, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 674 Loss: tensor(0.5071, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 675 Loss: tensor(0.4870, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 676 Loss: tensor(0.4677, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 677 Loss: tensor(0.4492, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 678 Loss: tensor(0.4314, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 679 Loss: tensor(0.4143, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 680 Loss: tensor(0.3979, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 681 Loss: tensor(0.3822, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 682 Loss: tensor(0.3670, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 683 Loss: tensor(0.3525, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 684 Loss: tensor(0.3385, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 685 Loss: tensor(0.3251, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 686 Loss: tensor(0.3123, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 687 Loss: tensor(0.2999, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 688 Loss: tensor(0.2880, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 689 Loss: tensor(0.2766, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 690 Loss: tensor(0.2657, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 691 Loss: tensor(0.2551, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 692 Loss: tensor(0.2450, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 693 Loss: tensor(0.2353, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 694 Loss: tensor(0.2260, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 695 Loss: tensor(0.2171, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 696 Loss: tensor(0.2085, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 697 Loss: tensor(0.2002, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 698 Loss: tensor(0.1923, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 699 Loss: tensor(0.1847, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 700 Loss: tensor(0.1774, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 701 Loss: tensor(0.1703, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 702 Loss: tensor(0.1636, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 703 Loss: tensor(0.1571, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 704 Loss: tensor(0.1509, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 705 Loss: tensor(0.1449, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 706 Loss: tensor(0.1392, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 707 Loss: tensor(0.1337, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 708 Loss: tensor(0.1284, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 709 Loss: tensor(0.1233, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 710 Loss: tensor(0.1184, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 711 Loss: tensor(0.1137, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 712 Loss: tensor(0.1092, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 713 Loss: tensor(0.1049, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 714 Loss: tensor(0.1007, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 715 Loss: tensor(0.0967, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 716 Loss: tensor(0.0929, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 717 Loss: tensor(0.0892, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 718 Loss: tensor(0.0857, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 719 Loss: tensor(0.0823, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 720 Loss: tensor(0.0790, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 721 Loss: tensor(0.0759, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 722 Loss: tensor(0.0729, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 723 Loss: tensor(0.0700, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 724 Loss: tensor(0.0673, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 725 Loss: tensor(0.0646, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 726 Loss: tensor(0.0620, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 727 Loss: tensor(0.0596, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 728 Loss: tensor(0.0572, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 729 Loss: tensor(0.0550, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 730 Loss: tensor(0.0528, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 731 Loss: tensor(0.0507, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 732 Loss: tensor(0.0487, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 733 Loss: tensor(0.0467, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 734 Loss: tensor(0.0449, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 735 Loss: tensor(0.0431, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 736 Loss: tensor(0.0414, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 737 Loss: tensor(0.0398, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 738 Loss: tensor(0.0382, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 739 Loss: tensor(0.0367, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 740 Loss: tensor(0.0352, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 741 Loss: tensor(0.0338, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 742 Loss: tensor(0.0325, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 743 Loss: tensor(0.0312, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 744 Loss: tensor(0.0300, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 745 Loss: tensor(0.0288, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 746 Loss: tensor(0.0276, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 747 Loss: tensor(0.0266, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 748 Loss: tensor(0.0255, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 749 Loss: tensor(0.0245, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 750 Loss: tensor(0.0235, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 751 Loss: tensor(0.0226, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 752 Loss: tensor(0.0217, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 753 Loss: tensor(0.0208, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 754 Loss: tensor(0.0200, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 755 Loss: tensor(0.0192, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 756 Loss: tensor(0.0185, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 757 Loss: tensor(0.0177, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 758 Loss: tensor(0.0170, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 759 Loss: tensor(0.0164, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 760 Loss: tensor(0.0157, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 761 Loss: tensor(0.0151, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 762 Loss: tensor(0.0145, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 763 Loss: tensor(0.0139, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 764 Loss: tensor(0.0134, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 765 Loss: tensor(0.0128, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 766 Loss: tensor(0.0123, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 767 Loss: tensor(0.0118, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 768 Loss: tensor(0.0114, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 769 Loss: tensor(0.0109, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 770 Loss: tensor(0.0105, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 771 Loss: tensor(0.0101, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 772 Loss: tensor(0.0097, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 773 Loss: tensor(0.0093, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 774 Loss: tensor(0.0089, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 775 Loss: tensor(0.0086, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 776 Loss: tensor(0.0082, grad_fn=<MseLossBackward0>)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 777 Loss: tensor(0.0079, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 778 Loss: tensor(0.0076, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 779 Loss: tensor(0.0073, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 780 Loss: tensor(0.0070, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 781 Loss: tensor(0.0067, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 782 Loss: tensor(0.0065, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 783 Loss: tensor(0.0062, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 784 Loss: tensor(0.0060, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 785 Loss: tensor(0.0057, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 786 Loss: tensor(0.0055, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 787 Loss: tensor(0.0053, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 788 Loss: tensor(0.0051, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 789 Loss: tensor(0.0049, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 790 Loss: tensor(0.0047, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 791 Loss: tensor(0.0045, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 792 Loss: tensor(0.0043, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 793 Loss: tensor(0.0041, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 794 Loss: tensor(0.0040, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 795 Loss: tensor(0.0038, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 796 Loss: tensor(0.0037, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 797 Loss: tensor(0.0035, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 798 Loss: tensor(0.0034, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 799 Loss: tensor(0.0032, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 800 Loss: tensor(0.0031, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 801 Loss: tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 802 Loss: tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 803 Loss: tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 804 Loss: tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 805 Loss: tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 806 Loss: tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 807 Loss: tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 808 Loss: tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 809 Loss: tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 810 Loss: tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 811 Loss: tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 812 Loss: tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 813 Loss: tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 814 Loss: tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 815 Loss: tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 816 Loss: tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 817 Loss: tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 818 Loss: tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 819 Loss: tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 820 Loss: tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 821 Loss: tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 822 Loss: tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 823 Loss: tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 824 Loss: tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 825 Loss: tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 826 Loss: tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 827 Loss: tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 828 Loss: tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 829 Loss: tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 830 Loss: tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 831 Loss: tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 832 Loss: tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 833 Loss: tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 834 Loss: tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 835 Loss: tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 836 Loss: tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 837 Loss: tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 838 Loss: tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 839 Loss: tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 840 Loss: tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 841 Loss: tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 842 Loss: tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 843 Loss: tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 844 Loss: tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 845 Loss: tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 846 Loss: tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 847 Loss: tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 848 Loss: tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 849 Loss: tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 850 Loss: tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 851 Loss: tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 852 Loss: tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 853 Loss: tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 854 Loss: tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 855 Loss: tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 856 Loss: tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 857 Loss: tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 858 Loss: tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 859 Loss: tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 860 Loss: tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 861 Loss: tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 862 Loss: tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 863 Loss: tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 864 Loss: tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 865 Loss: tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 866 Loss: tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 867 Loss: tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 868 Loss: tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 869 Loss: tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 870 Loss: tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 871 Loss: tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 872 Loss: tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 873 Loss: tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 874 Loss: tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 875 Loss: tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 876 Loss: tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 877 Loss: tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 878 Loss: tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 879 Loss: tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 880 Loss: tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 881 Loss: tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 882 Loss: tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 883 Loss: tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 884 Loss: tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 885 Loss: tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 886 Loss: tensor(9.6596e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 887 Loss: tensor(9.2773e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 888 Loss: tensor(8.9099e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 889 Loss: tensor(8.5570e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 890 Loss: tensor(8.2181e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 891 Loss: tensor(7.8929e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 892 Loss: tensor(7.5804e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 893 Loss: tensor(7.2804e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 894 Loss: tensor(6.9920e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 895 Loss: tensor(6.7153e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 896 Loss: tensor(6.4495e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 897 Loss: tensor(6.1940e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 898 Loss: tensor(5.9488e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 899 Loss: tensor(5.7132e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 900 Loss: tensor(5.4870e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 901 Loss: tensor(5.2698e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 902 Loss: tensor(5.0611e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 903 Loss: tensor(4.8607e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 904 Loss: tensor(4.6681e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 905 Loss: tensor(4.4833e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 906 Loss: tensor(4.3057e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 907 Loss: tensor(4.1353e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 908 Loss: tensor(3.9717e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 909 Loss: tensor(3.8143e-05, grad_fn=<MseLossBackward0>)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 910 Loss: tensor(3.6633e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 911 Loss: tensor(3.5181e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 912 Loss: tensor(3.3787e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 913 Loss: tensor(3.2448e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 914 Loss: tensor(3.1162e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 915 Loss: tensor(2.9929e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 916 Loss: tensor(2.8744e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 917 Loss: tensor(2.7605e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 918 Loss: tensor(2.6511e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 919 Loss: tensor(2.5461e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 920 Loss: tensor(2.4453e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 921 Loss: tensor(2.3485e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 922 Loss: tensor(2.2554e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 923 Loss: tensor(2.1661e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 924 Loss: tensor(2.0804e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 925 Loss: tensor(1.9980e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 926 Loss: tensor(1.9188e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 927 Loss: tensor(1.8430e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 928 Loss: tensor(1.7700e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 929 Loss: tensor(1.6999e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 930 Loss: tensor(1.6325e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 931 Loss: tensor(1.5679e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 932 Loss: tensor(1.5058e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 933 Loss: tensor(1.4461e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 934 Loss: tensor(1.3888e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 935 Loss: tensor(1.3338e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 936 Loss: tensor(1.2810e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 937 Loss: tensor(1.2303e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 938 Loss: tensor(1.1817e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 939 Loss: tensor(1.1349e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 940 Loss: tensor(1.0899e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 941 Loss: tensor(1.0467e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 942 Loss: tensor(1.0054e-05, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 943 Loss: tensor(9.6554e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 944 Loss: tensor(9.2725e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 945 Loss: tensor(8.9060e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 946 Loss: tensor(8.5538e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 947 Loss: tensor(8.2155e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 948 Loss: tensor(7.8907e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 949 Loss: tensor(7.5778e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 950 Loss: tensor(7.2776e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 951 Loss: tensor(6.9898e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 952 Loss: tensor(6.7127e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 953 Loss: tensor(6.4473e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 954 Loss: tensor(6.1920e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 955 Loss: tensor(5.9465e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 956 Loss: tensor(5.7105e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 957 Loss: tensor(5.4849e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 958 Loss: tensor(5.2682e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 959 Loss: tensor(5.0591e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 960 Loss: tensor(4.8584e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 961 Loss: tensor(4.6659e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 962 Loss: tensor(4.4813e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 963 Loss: tensor(4.3035e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 964 Loss: tensor(4.1331e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 965 Loss: tensor(3.9690e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 966 Loss: tensor(3.8119e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 967 Loss: tensor(3.6608e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 968 Loss: tensor(3.5153e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 969 Loss: tensor(3.3764e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 970 Loss: tensor(3.2428e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 971 Loss: tensor(3.1144e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 972 Loss: tensor(2.9911e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 973 Loss: tensor(2.8727e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 974 Loss: tensor(2.7591e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 975 Loss: tensor(2.6501e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 976 Loss: tensor(2.5449e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 977 Loss: tensor(2.4439e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 978 Loss: tensor(2.3473e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 979 Loss: tensor(2.2540e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 980 Loss: tensor(2.1647e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 981 Loss: tensor(2.0792e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 982 Loss: tensor(1.9969e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 983 Loss: tensor(1.9175e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 984 Loss: tensor(1.8417e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 985 Loss: tensor(1.7686e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 986 Loss: tensor(1.6983e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 987 Loss: tensor(1.6313e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 988 Loss: tensor(1.5667e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 989 Loss: tensor(1.5047e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 990 Loss: tensor(1.4451e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 991 Loss: tensor(1.3877e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 992 Loss: tensor(1.3327e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 993 Loss: tensor(1.2798e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 994 Loss: tensor(1.2291e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 995 Loss: tensor(1.1804e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 996 Loss: tensor(1.1338e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 997 Loss: tensor(1.0890e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 998 Loss: tensor(1.0457e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 999 Loss: tensor(1.0042e-06, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "Epoch: 1000 Loss: tensor(9.6441e-07, grad_fn=<MseLossBackward0>)\n",
      "\n",
      "<generator object Module.parameters at 0x0000019701B7D510>\n"
     ]
    }
   ],
   "source": [
    "#test below\n",
    "x = torch.load(\"C:/Users/nikce/anaconda3/Project/images/inputVars.pt\")\n",
    "y = torch.load(\"C:/Users/nikce/anaconda3/Project/images/outputVars.pt\")\n",
    "\n",
    "x = x[0:20]\n",
    "y = y[0:20]\n",
    "\n",
    "x = x.reshape(20, 1, 200, 407)\n",
    "y = y.reshape(20, 1)\n",
    "\n",
    "\n",
    "model = CNN(200*407, 1)  # pass the input and output sizes to the model\n",
    "\n",
    "train_loop(1000, .01, model, x, y)\n",
    "print(model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "fb928644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0010]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.load(\"C:/Users/nikce/anaconda3/Project/images/inputVars.pt\")\n",
    "y_pred = model.forward(x[100].reshape(1, 1, 200, 407))\n",
    "print(y_pred)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d019e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c7cbdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
